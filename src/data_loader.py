import tensorflow as tf
from tensorflow.keras.preprocessing import image_dataset_from_directory

def load_datasets(data_dir, img_size=(224, 224), batch_size=32, seed=42):
    """
    Load v√† optimize datasets v·ªõi prefetch, cache, v√† shuffle.
    
    Args:
        data_dir: ƒê∆∞·ªùng d·∫´n t·ªõi th∆∞ m·ª•c dataset
        img_size: K√≠ch th∆∞·ªõc ·∫£nh ƒë·∫ßu ra (default: 224x224)
        batch_size: S·ªë ·∫£nh m·ªói batch (default: 32)
        seed: Random seed cho reproducibility (default: 42)
    
    Returns:
        train_ds, val_ds, class_names
    """
    train_dir = f"{data_dir}/train"
    val_dir = f"{data_dir}/val"
    
    # Load datasets v·ªõi seed ƒë·ªÉ reproducible
    train_ds = image_dataset_from_directory(
        train_dir,
        image_size=img_size,
        batch_size=batch_size,
        label_mode='categorical',
        shuffle=True,
        seed=seed
    )
    
    val_ds = image_dataset_from_directory(
        val_dir,
        image_size=img_size,
        batch_size=batch_size,
        label_mode='categorical',
        shuffle=False
    )
    
    # üëâ L·∫•y class_names ngay t·∫°i ƒë√¢y
    class_names = train_ds.class_names
    
    # Performance optimization
    AUTOTUNE = tf.data.AUTOTUNE
    
    train_ds = train_ds.cache().shuffle(1000, seed=seed).prefetch(AUTOTUNE)
    val_ds = val_ds.cache().prefetch(AUTOTUNE)
    
    print(f"‚úÖ Loaded {len(class_names)} classes: {class_names}")
    print(f"‚úÖ Training batches: {len(train_ds)}")
    print(f"‚úÖ Validation batches: {len(val_ds)}")
    
    return train_ds, val_ds, class_names
